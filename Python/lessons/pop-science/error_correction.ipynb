{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Коррекция ошибок\n",
    "\n",
    "Все мы постоянно печатаем, постянно совершаем опечатки, для нас коррекция ошибок уже давно стала обыденностью. Как же она может работать? Что значит _слова похожи_? Какие неожиданные применения автокоррекция находит себе?  \n",
    "Впереди нас ожидает целый шпионский триллер!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## В поисках Aшибки\n",
    "\n",
    "Стоя перед задачей исправления ошибки в слове, сразу возникают несколько проблем:\n",
    "\n",
    "-   Что значит, что в 'в слове ошибка'?\n",
    "-   Как найти ошибку?\n",
    "-   Как исправлять?\n",
    "-   Из чего подбирать замену?\n",
    "\n",
    "В ходе нашего небольшого расследования мы увидимм ответы, накопивштеся за годы в разных сферах применения. А начнём мы с простейшего подхода - словари и частотные таблицы.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-Список', 'продуктов-', 'Куриное', 'филе', 'Куруза']\n"
     ]
    }
   ],
   "source": [
    "# Если мы не знаем слово, значит просто его нет!\n",
    "# Такой девиз вполне себе применим к задаче исправления человеческого текста\n",
    "\n",
    "russian_dictionary = ['капуста', 'крыжовник', 'картофель', 'кукуруза', 'кинза']\n",
    "\n",
    "shoplist = \"\"\"\n",
    "    -Список продуктов-\n",
    "    Капуста\n",
    "    Куриное филе\n",
    "    Куруза\n",
    "    Кинза\n",
    "\"\"\"\n",
    "\n",
    "errors = [\n",
    "    word\n",
    "    for word in shoplist.split()\n",
    "    if word.lower() not in russian_dictionary\n",
    "]\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нуу, существует проблема того, что размер словаря имеет значение... Зато мы засекли `куруза`!  \n",
    "На удивление просто! Взять бы словарь побольше и достоверность нашего поиска ошибок увеличится. Другое дело как определять где ошибка, а главное, как их исправлять?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слова в тексте: ['список', 'продуктов', 'капуста', 'куриное', 'филе', 'куруза', 'кинза']\n",
      "Слова с ошибками: куруза\n"
     ]
    }
   ],
   "source": [
    "shoplist = \"\"\"\n",
    "    -Список продуктов-\n",
    "    Капуста\n",
    "    Куриное филе\n",
    "    Куруза\n",
    "    Кинза\n",
    "\"\"\"\n",
    "\n",
    "# Улучшим наше положение\n",
    "# Зарание для коррекции ошибок разобьём текст на слова только из букв и ничего больше\n",
    "# str.isalpha() возврашает истину только если обьект str состоить из букв\n",
    "def tokenize(string: str):\n",
    "    # побуквенно выкинем небуквы, а потом склеим и в нижний регистр\n",
    "    tokens = [\n",
    "        ''.join(filter(str.isalpha, word)).lower() \n",
    "        for word in string.split()\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "shoplist_tokenized = tokenize(shoplist)\n",
    "print('Слова в тексте:', shoplist_tokenized)\n",
    "\n",
    "# Словарь русского языка, содержит 1 531 464 записи\n",
    "russian_dictionary = []\n",
    "with open(\"../assets/russian.txt\", mode='r') as dictionary:\n",
    "    russian_dictionary = dictionary.read().split()\n",
    "\n",
    "errors = [\n",
    "        word for word in shoplist_tokenized\n",
    "        if word not in russian_dictionary\n",
    "    ]\n",
    "print('Слова с ошибками:', *errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так-то лучше! Ошибку мы так и не исправили, но теперь мы достоверно её отыскали!  \n",
    "Недостатки? Словарь просто гигантский! (по меркам нашей небольшой программки) Он весит 30Мб!\n",
    "\n",
    "Хорошо, раз у нас уже есть словарь, давайте выбирать замену оттуда. Как подбирать замену? Наверное надо найти самое похожее слово и подменить на него. Как измерять похожесть? Проведём исследование.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Расстояние Хэмминга\n",
    "\n",
    "Ричард Хэмминг - американский инжинер, программист, учёный. Прославился благодаря своему огромному вкладу в теорию помехойстойчивого кодирования. В честь него названы две вещи: Коды Хэмминга и Расстояние Хэмминга.\n",
    "\n",
    "Мы ещё поговорим про самокорректирующие коды Хэмминга, а сейчас нас интересует Расстояние Хэмминга.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "# Расстояние Хэмминга - количество позиций, в которых два слова одинаковой длины отличаются\n",
    "def hamming_distance(w1, w2):\n",
    "    if len(w1) != len(w2): # разная длинна -> бесконечное расстояние\n",
    "        return float('inf')\n",
    "    return sum(a != b for a, b in zip(w1, w2))\n",
    "\n",
    "print(hamming_distance('cat', 'bat')) # c != b - расстояние 1\n",
    "print(hamming_distance('luke', 'wake'))\n",
    "print(hamming_distance('крут', 'круто')) # а вот и слабое место, слова разной длинны несравнимы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 'куруза')]\n",
      "[('буруна', 2), ('бутуза', 2), ('кляуза', 2), ('комуза', 2), ('курага', 2)]\n",
      "[('куража', 2), ('курдка', 2), ('курева', 2), ('курила', 2), ('курица', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Вынесем нахождение ошибок в наборе нокенов в отдельную функцию\n",
    "def find_errors(tokens, dictionary):\n",
    "    return [\n",
    "        (i, word) for i, word in enumerate(tokens) # до кучи будем запоминать позицию, в которой найдена ошибка\n",
    "        if word not in dictionary\n",
    "    ]\n",
    "\n",
    "# Проверим расстояние Хэмминга - найдём ошибку и попробуем подыскать замену\n",
    "\n",
    "errors = find_errors(shoplist_tokenized, russian_dictionary)\n",
    "print(errors)\n",
    "\n",
    "corrections = []\n",
    "for i, wrong_word in errors:\n",
    "    # словарь гигантский а значит, чтобы не нагружать себя обработкой всего словаря\n",
    "    # обернём всё в () - получим генератор вместо списка, он сам по себе ничего не содержит\n",
    "    # вместо этого он высчитывает свои элементы в момент, когда мы будем итерировать по нему\n",
    "    candidates = (\n",
    "        (replacement, dist) \n",
    "        for replacement, dist in \n",
    "        map(lambda w: (w, hamming_distance(w, wrong_word)), russian_dictionary)\n",
    "        if dist != float('inf') # бесконечно удалённые нас не интересуют\n",
    "    )\n",
    "    first_10 = sorted(candidates, key=lambda a: a[1])[:10]\n",
    "    print(first_10[:5], first_10[5:], sep='\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея замечательная, имя Хэмминга не вошло бы в историю, если бы была плохой. Да вот только ошибку в слове `кукуруза` мы и близко не найдём т.к. мы потеряли буквы, а расстяние хэмминга в таком случае неприменимо. Ну давайте перед тем, как отправиться искать другой путь, всё же смоделируем более благоприятный исход и просто наделаем опечаток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_corrections(located_errors, dictionary, distance_function, first_n=10):\n",
    "    result = []\n",
    "    for i, wrong_word in located_errors:\n",
    "        candidates = (\n",
    "            (replacement, dist) \n",
    "            for replacement, dist in \n",
    "            map(lambda w: (w, distance_function(w, wrong_word)), dictionary)\n",
    "            if dist != float('inf') \n",
    "        )\n",
    "        candidates = sorted(candidates, key=lambda a: a[1])\n",
    "        result.append((\n",
    "            i, \n",
    "            wrong_word,\n",
    "            list(zip(*candidates))[0][:first_n]\n",
    "        ))\n",
    "    return result\n",
    "\n",
    "test_string = \"Я скушел многа аблок, и аблоки были очень вкусными!\"\n",
    "test_string_tokenized = tokenize(test_string)\n",
    "test_errors = find_errors(test_string_tokenized, russian_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'скушел', ('скушал', 'скушен', 'акушер', 'вкушал', 'сдурел', 'скошен', 'скуден', 'скулеж', 'скулил', 'скупал'))\n",
      "(2, 'многа', ('многи', 'много', 'множа', 'анода', 'блога', 'вноса', 'гнома', 'грога', 'дрога', 'енола'))\n",
      "(3, 'аблок', ('облок', 'яблок', 'абрек', 'авлос', 'аллод', 'аплод', 'аулов', 'аулом', 'балок', 'белок'))\n",
      "(5, 'аблоки', ('яблоки', 'абреки', 'волоки', 'молоки', 'облаки', 'облеки', 'облоги', 'обложи', 'обломи', 'оброки'))\n"
     ]
    }
   ],
   "source": [
    "test_suggestions = suggest_corrections(test_errors, russian_dictionary, hamming_distance)\n",
    "print(*test_suggestions, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гораздо лучше! В каждом списке есть правильное предложение! Жаль, что мы замену четырёх слов искали 3.3 секунды... давайте поправим наше решение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'скушел', ('скушал', 'скушен', 'акушер', 'вкушал', 'сдурел', 'скошен', 'скуден', 'скулеж', 'скулил', 'скупал'))\n",
      "(2, 'многа', ('многи', 'много', 'множа', 'анода', 'блога', 'вноса', 'гнома', 'грога', 'дрога', 'енола'))\n",
      "(3, 'аблок', ('облок', 'яблок', 'абрек', 'авлос', 'аллод', 'аплод', 'аулов', 'аулом', 'балок', 'белок'))\n",
      "(5, 'аблоки', ('яблоки', 'абреки', 'волоки', 'молоки', 'облаки', 'облеки', 'облоги', 'обложи', 'обломи', 'оброки'))\n"
     ]
    }
   ],
   "source": [
    "def suggest_corrections(located_errors, dictionary, distance_function, first_n=10, len_margin=4):\n",
    "    result = []\n",
    "    # ограничимся словами в непосредственной близости к словам в наличии\n",
    "    min_len = len(min(located_errors, key=lambda a: a[1])[1]) # минимальная длина слова с ошибкой\n",
    "    min_len_ = max(1, min_len - len_margin) # отрицательной длины не бывает\n",
    "    max_len = len(max(located_errors, key=lambda a: a[1])[1]) + len_margin\n",
    "    \n",
    "    filtered_dictionary = list(filter(\n",
    "        lambda word: len(word) >= min_len and len(word) <= max_len,\n",
    "        dictionary\n",
    "    ))\n",
    "    for i, wrong_word in located_errors:\n",
    "        candidates = (\n",
    "            (replacement, dist) \n",
    "            for replacement, dist in \n",
    "            map(lambda w: (w, distance_function(w, wrong_word)), filtered_dictionary)\n",
    "            if dist != float('inf') \n",
    "        )\n",
    "        candidates = sorted(candidates, key=lambda a: a[1])\n",
    "        result.append((\n",
    "            i, \n",
    "            wrong_word,\n",
    "            list(zip(*candidates))[0][:first_n]\n",
    "        ))\n",
    "    return result\n",
    "\n",
    "test_suggestions = suggest_corrections(test_errors, russian_dictionary, hamming_distance)\n",
    "print(*test_suggestions, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сэкономили секунду.   \n",
    "Поехали дальше. Нам нужна другая функция расстояния, желательно чтобы она учитывала возможность удаления и вставки букв, а не только замены одних на другие. Такую уже придумали.\n",
    "\n",
    "## Расстояние Левенштейна\n",
    "\n",
    "Владимир Иосифович Левенштейн - советский и российский математик, доктор физико-математических наук. Ведущий научный сотрудник Института прикладной математики им. М. В. Келдыша.  \n",
    "\n",
    "В 1965 году ввёл понятие дистанции редактирования, ныне называемой в честь него расстоянием Левенштейна. Эта метрика учитывает как и перестановки букв, так и вставки с удалениями, что делает её практически более применимой к коррекции человеческих ошибок.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache\n",
    "def levenshtein_distance(w1, w2):\n",
    "    if len(w1) == 0: # разница в длинне теперь учитывается в пользу расстояния\n",
    "        return len(w2)\n",
    "    if len(w2) == 0:\n",
    "        return len(w1)\n",
    "    if w1[0] == w2[0]: # очевидно, если начало одинаковое, то сравниваем концы\n",
    "        return levenshtein_distance(w1[1:], w2[1:])\n",
    "    _1 = levenshtein_distance(w1[1:], w2) # удаление буквы\n",
    "    _2 = levenshtein_distance(w2[1:], w1) # вставка буквы\n",
    "    _3 = levenshtein_distance(w1[1:], w2[1:]) # замена буквы\n",
    "    return 1 + min(_1, _2, _3)\n",
    "\n",
    "print(levenshtein_distance('павел', 'щавель'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['список', 'продуктов', 'капуста', 'куриное', 'филе', 'куруза', 'кинза']\n",
      "[(5, 'куруза')]\n",
      "(5, 'куруза', ('буруна', 'бутуза', 'картуза', 'кляуза', 'комуза', 'круиза', 'кукуруза', 'курага', 'куража', 'кургузая'))\n"
     ]
    }
   ],
   "source": [
    "# наконец мы сможем отыскать ошибки в списке покупок\n",
    "print(shoplist_tokenized)\n",
    "shoplist_errors = find_errors(shoplist_tokenized, russian_dictionary)\n",
    "print(shoplist_errors)\n",
    "shoplist_suggestions = suggest_corrections(shoplist_errors, russian_dictionary, levenshtein_distance, len_margin=2)\n",
    "print(*shoplist_suggestions, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30 секунд поиска на одно слово, и это при том, что мы кэшировали вычисления и ограничились 2 буквами разницы в длине!!  \n",
    "\n",
    "Ну зато среди всех слов мы нашли `кукуруза` :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Улучшим реализациию \n",
    "@lru_cache\n",
    "def iterative_levenshtein_distance(w1, w2):\n",
    "    n = len(w2)\n",
    "    m = len(w1)\n",
    "    v0 = np.ndarray(n + 1)\n",
    "    v1 = np.ndarray(n + 1)\n",
    "\n",
    "    for i in range(n + 1):\n",
    "        v0[i] = i\n",
    "\n",
    "    for i in range(m):\n",
    "        v1[0] = i + 1\n",
    "        for j in range(n):\n",
    "            deletion_cost = v0[j + 1] + 1\n",
    "            insertion_cost = v1[j] + 1\n",
    "            substitution_cost = v0[j] if w1[i] == w2[j] else v0[j] + 1\n",
    "            v1[j+1] = min(deletion_cost, insertion_cost, substitution_cost)\n",
    "        v0, v1 = v1, v0\n",
    "\n",
    "    return v0[n]\n",
    "                \n",
    "d = iterative_levenshtein_distance('павел', 'щавель')\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 'куруза', ('буруна', 'бутуза', 'картуза', 'кляуза', 'комуза', 'круиза', 'кукуруза', 'курага', 'куража', 'кургузая'))\n"
     ]
    }
   ],
   "source": [
    "shoplist_suggestions = suggest_corrections(shoplist_errors, russian_dictionary, iterative_levenshtein_distance, len_margin=2)\n",
    "print(*shoplist_suggestions, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почти в три раза быстрее! Это достижение сильно улучшает применимость алгоритма. Давайте напоследок попробуем наши яблоки с расстоянием Левенштейна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'скушел', ('скушал', 'скушен', 'акушер', 'вкушал', 'искушал', 'искушен', 'оскудел', 'сдурел', 'скошен', 'скудели', 'скудель', 'скуден', 'скулеж', 'скулил', 'скупал'))\n",
      "(2, 'многа', ('минога', 'многая', 'иногда', 'магога', 'миногам', 'миногах', 'миноге', 'миноги', 'миногу', 'минора', 'многие', 'многий', 'многим', 'многих', 'многое'))\n",
      "(3, 'йаблок', ('гакблок', 'заулок', 'каблук', 'наблой', 'наблою', 'шаблон', 'арабок', 'бабенк', 'бабенок', 'бабёнок', 'бабкой', 'бабкою', 'бабник', 'бабулек', 'бабушк'))\n"
     ]
    }
   ],
   "source": [
    "test_string = \"Я скушел многа йаблок, и яблоки были очень вкусными!\"\n",
    "test_string_tokenized = tokenize(test_string)\n",
    "test_errors = find_errors(test_string_tokenized, russian_dictionary)\n",
    "test_suggestions = suggest_corrections(test_errors, russian_dictionary, iterative_levenshtein_distance, len_margin=2, first_n=15)\n",
    "print(*test_suggestions, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В половине выборок мы не нашли правильной замены. Слабость расстояния Левенштейна в том, что замены весят столько же сколько и вставки и удаления, хотя доказано, что большинство совершаемых людьми ошибок - опечатки. Выходит, что мы используя более сложный алгоритм не можем найти подходящих слов.\n",
    "\n",
    "На самом деле, для улучшения времени выполнения нам стоит:  \n",
    "- Сократить словарь, ограничимся самыми частыми словами (Раза в 2-3 было бы хорошо)\n",
    "- Реализовать потоковую выдачу предложений, это важный пункт для интерактивных сред\n",
    "- Отсортировать словарь по частотам встречающихся слов"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
